# This is the feature selection module

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_classif

# Define a function to select features from the data


def select_features(data, target, k=10, method='chi2'):
    # Choose the score function based on the method
    if method == 'chi2':
        score_func = chi2
    elif method == 'mutual_info':
        score_func = mutual_info_classif
    elif method == 'f_classif':
        score_func = f_classif
    else:
        raise ValueError('Invalid method: {}'.format(method))

    # Use SelectKBest to select the top k features
    selector = SelectKBest(score_func=score_func, k=k)
    selector.fit(data, target)
    selected_features = data.columns[selector.get_support()]
    return selected_features

# Define a function to select features based on correlation


def select_features_correlation(data, target, threshold=0.5):
    # Calculate the correlation matrix
    corr_matrix = data.corr().abs()
    # Select upper triangle of correlation matrix
    upper = corr_matrix.where(
        np.triu(
            np.ones(
                corr_matrix.shape),
            k=1).astype(
            np.bool))
    # Find features with correlation greater than threshold
    to_drop = [
        column for column in upper.columns if any(
            upper[column] > threshold)]
    # Drop features
    data.drop(to_drop, axis=1, inplace=True)
    return data

# Define a function to select features based on variance


def select_features_variance(data, threshold=0.5):
    # Calculate the variance
    variances = data.var()
    # Select features with variance greater than threshold
    selected_features = variances[variances > threshold].index
    return selected_features

# Define a function to select features based on mutual information


def select_features_mutual_info(data, target, k=10):
    # Calculate the mutual information
    mutual_info = mutual_info_classif(data, target)
    # Select the top k features
    selected_features = data.columns[np.argsort(mutual_info)[-k:]]
    return selected_features

# Define a function to select features based on chi-squared test


def select_features_chi2(data, target, k=10):
    # Calculate the chi-squared stats
    chi2_stats = chi2(data, target)
    # Select the top k features
    selected_features = data.columns[np.argsort(chi2_stats[0])[-k:]]
    return selected_features

# Define a function to select features based on Recursive Feature Elimination


def select_features_rfe(data, target, estimator, k=10):
    from sklearn.feature_selection import RFE
    # Initialize RFE
    rfe = RFE(estimator, n_features_to_select=k)
    rfe.fit(data, target)
    selected_features = data.columns[rfe.support_]
    return selected_features

# Define a function to select features based on Feature Importance


def select_features_importance(data, target, estimator):
    # Fit the estimator
    estimator.fit(data, target)
    # Get feature importances
    importances = estimator.feature_importances_
    # Select features with importance greater than the mean importance
    selected_features = data.columns[importances > np.mean(importances)]
    return selected_features

# Define a function to select features based on PCA


def select_features_pca(data, k=10):
    from sklearn.decomposition import PCA
    # Initialize PCA
    pca = PCA(n_components=k)
    # Fit PCA
    pca.fit(data)
    # Transform the data
    transformed_data = pca.transform(data)
    return transformed_data

# Define a function to select features based on Lasso


def select_features_lasso(data, target, alpha=0.1):
    from sklearn.linear_model import LassoCV
    # Initialize LassoCV
    lasso = LassoCV(alphas=[alpha])
    # Fit LassoCV
    lasso.fit(data, target)
    # Select features where the coefficient is non-zero
    selected_features = data.columns[lasso.coef_ != 0]
    return selected_features
